<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · KernelForge.jl</title><meta name="title" content="API Reference · KernelForge.jl"/><meta property="og:title" content="API Reference · KernelForge.jl"/><meta property="twitter:title" content="API Reference · KernelForge.jl"/><meta name="description" content="Documentation for KernelForge.jl."/><meta property="og:description" content="Documentation for KernelForge.jl."/><meta property="twitter:description" content="Documentation for KernelForge.jl."/><meta property="og:url" content="https://epilliat.github.io/KernelForge.jl/stable/api/"/><meta property="twitter:url" content="https://epilliat.github.io/KernelForge.jl/stable/api/"/><link rel="canonical" href="https://epilliat.github.io/KernelForge.jl/stable/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">KernelForge.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../performances/">Performance</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Copy"><span>Copy</span></a></li><li><a class="tocitem" href="#Map-Reduce"><span>Map-Reduce</span></a></li><li><a class="tocitem" href="#Scan"><span>Scan</span></a></li><li><a class="tocitem" href="#Matrix-Vector"><span>Matrix-Vector</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/epilliat/KernelForge.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/epilliat/KernelForge.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><h2 id="Copy"><a class="docs-heading-anchor" href="#Copy">Copy</a><a id="Copy-1"></a><a class="docs-heading-anchor-permalink" href="#Copy" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.vcopy!"><a class="docstring-binding" href="#KernelForge.vcopy!"><code>KernelForge.vcopy!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">vcopy!(dst::AbstractGPUVector, src::AbstractGPUVector; Nitem=4)</code></pre><p>Copy <code>src</code> to <code>dst</code> using vectorized GPU memory access.</p><p>Performs a high-throughput copy by loading and storing <code>Nitem</code> elements per thread, reducing memory transaction overhead compared to scalar copies.</p><p><strong>Arguments</strong></p><ul><li><code>dst</code>: Destination GPU vector</li><li><code>src</code>: Source GPU vector (must have same length as <code>dst</code>)</li><li><code>Nitem=4</code>: Number of elements processed per thread. Higher values improve throughput but require <code>length(src)</code> to be divisible by <code>Nitem</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">src = CUDA.rand(Float32, 1024)
dst = CUDA.zeros(Float32, 1024)
vcopy!(dst, src)</code></pre><p>See also: <a href="#KernelForge.setvalue!"><code>KernelForge.setvalue!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/copy/copy.jl#L1-L23">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.setvalue!"><a class="docstring-binding" href="#KernelForge.setvalue!"><code>KernelForge.setvalue!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">setvalue!(dst::AbstractGPUVector{T}, val::T; Nitem=4) where T</code></pre><p>Fill <code>dst</code> with <code>val</code> using vectorized GPU memory access.</p><p>Performs a high-throughput fill by storing <code>Nitem</code> copies of <code>val</code> per thread, reducing memory transaction overhead compared to scalar writes.</p><p><strong>Arguments</strong></p><ul><li><code>dst</code>: Destination GPU vector</li><li><code>val</code>: Value to fill (must match element type of <code>dst</code>)</li><li><code>Nitem=4</code>: Number of elements written per thread. Higher values improve throughput but require <code>length(dst)</code> to be divisible by <code>Nitem</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">dst = CUDA.zeros(Float32, 1024)
setvalue!(dst, 1.0f0)</code></pre><p>See also: <a href="#KernelForge.vcopy!"><code>KernelForge.vcopy!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/copy/copy.jl#L30-L51">source</a></section></details></article><h2 id="Map-Reduce"><a class="docs-heading-anchor" href="#Map-Reduce">Map-Reduce</a><a id="Map-Reduce-1"></a><a class="docs-heading-anchor-permalink" href="#Map-Reduce" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce"><a class="docstring-binding" href="#KernelForge.mapreduce"><code>KernelForge.mapreduce</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce(f, op, src::AbstractGPUArray; dims=nothing, kwargs...) -&gt; Array or scalar</code></pre><p>GPU parallel map-reduce operation with optional dimension reduction.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>dims=nothing</code>: Dimensions to reduce over. Options:<ul><li><code>nothing</code> or <code>:</code>: Reduce over all dimensions (returns scalar or 1-element array)</li><li><code>Int</code>: Reduce over single dimension</li><li><code>Tuple{Int...}</code>: Reduce over multiple dimensions (must be contiguous from start or end)</li></ul></li><li><code>g=identity</code>: Post-reduction transformation</li><li><code>init=nothing</code>: Initial value (currently unused, for API compatibility)</li><li><code>to_cpu=false</code>: If true and <code>dims=nothing</code>, return scalar; otherwise return GPU array</li><li>Additional kwargs passed to underlying implementations</li></ul><p><strong>Dimension Constraints</strong></p><p>The <code>dims</code> argument must specify contiguous dimensions from either:</p><ul><li>The beginning: <code>(1,)</code>, <code>(1,2)</code>, <code>(1,2,3)</code>, etc.</li><li>The end: <code>(n-1,n)</code>, <code>(n,)</code>, etc. for an n-dimensional array</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 100, 50, 20)

# Full reduction (all dimensions)
total = mapreduce(identity, +, A; to_cpu=true)

# Reduce along dim 1: (100, 50, 20) -&gt; (50, 20)
col_sums = mapreduce(identity, +, A; dims=1)

# Reduce along dims (1,2): (100, 50, 20) -&gt; (20,)
plane_sums = mapreduce(identity, +, A; dims=(1,2))

# Reduce along last dim: (100, 50, 20) -&gt; (100, 50)
depth_sums = mapreduce(identity, +, A; dims=3)

# Reduce along last two dims: (100, 50, 20) -&gt; (100,)
slice_sums = mapreduce(identity, +, A; dims=(2,3))</code></pre><p>See also: <a href="#KernelForge.mapreduce!"><code>KernelForge.mapreduce!</code></a>, <a href="#KernelForge.mapreduce1d"><code>mapreduce1d</code></a>, <a href="#KernelForge.mapreduce2d"><code>mapreduce2d</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/mapreduce.jl#L5-L51">source</a></section><section><div><pre><code class="language-julia hljs">mapreduce(f, op, srcs::NTuple{N,AbstractGPUArray}; dims=nothing, kwargs...)</code></pre><p>Multi-array mapreduce. Only supports full reduction (dims=nothing).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/mapreduce.jl#L380-L384">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce!"><a class="docstring-binding" href="#KernelForge.mapreduce!"><code>KernelForge.mapreduce!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce!(f, op, dst, src; dims=nothing, kwargs...)</code></pre><p>In-place GPU parallel map-reduce with dimension support.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>dst</code>: Output array</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>dims=nothing</code>: Dimensions to reduce over (see <code>mapreduce</code> for details)</li><li><code>g=identity</code>: Post-reduction transformation</li><li>Additional kwargs passed to underlying implementations</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 100, 50)
col_sums = CUDA.zeros(Float32, 50)
row_sums = CUDA.zeros(Float32, 100)

# Column sums (reduce dim 1)
mapreduce!(identity, +, col_sums, A; dims=1)

# Row sums (reduce dim 2)
mapreduce!(identity, +, row_sums, A; dims=2)</code></pre><p>See also: <a href="#KernelForge.mapreduce"><code>KernelForge.mapreduce</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/mapreduce.jl#L102-L132">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce2d"><a class="docstring-binding" href="#KernelForge.mapreduce2d"><code>KernelForge.mapreduce2d</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce2d(f, op, src, dim; kwargs...) -&gt; Vector</code></pre><p>GPU parallel reduction along dimension <code>dim</code>.</p><ul><li><code>dim=1</code>: Column-wise reduction (vertical), output length = number of columns</li><li><code>dim=2</code>: Row-wise reduction (horizontal), output length = number of rows</li></ul><p><strong>Arguments</strong></p><ul><li><code>f</code>: Element-wise transformation</li><li><code>op</code>: Reduction operator</li><li><code>src</code>: Input matrix of size <code>(n, p)</code></li><li><code>dim</code>: Dimension to reduce along (1 or 2)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p>For <code>dim=1</code> (column-wise):</p><ul><li><code>Nitem=nothing</code>: Items per thread</li><li><code>Nthreads=nothing</code>: Threads per column reduction</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks=nothing</code>: Number of blocks</li></ul><p>For <code>dim=2</code> (row-wise):</p><ul><li><code>chunksz=nothing</code>: Chunk size for row processing</li><li><code>Nblocks=nothing</code>: Number of blocks per row</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks_row=nothing</code>: Blocks per row</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)

# Column sums (reduce along dim=1)
col_sums = mapreduce2d(identity, +, A, 1)

# Row maximums (reduce along dim=2)
row_maxs = mapreduce2d(identity, max, A, 2)

# Column means
col_means = mapreduce2d(identity, +, A, 1; g=x -&gt; x / size(A, 1))

# Sum of squares per row
row_ss = mapreduce2d(abs2, +, A, 2)</code></pre><p>See also: <a href="#KernelForge.mapreduce2d!"><code>KernelForge.mapreduce2d!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/2D/mapreduce2d.jl#L1-L50">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce2d!"><a class="docstring-binding" href="#KernelForge.mapreduce2d!"><code>KernelForge.mapreduce2d!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce2d!(f, op, dst, src, dim; kwargs...)</code></pre><p>In-place GPU parallel reduction along dimension <code>dim</code>.</p><ul><li><code>dim=1</code>: Column-wise reduction (vertical), <code>dst</code> length = number of columns</li><li><code>dim=2</code>: Row-wise reduction (horizontal), <code>dst</code> length = number of rows</li></ul><p><strong>Arguments</strong></p><ul><li><code>f</code>: Element-wise transformation</li><li><code>op</code>: Reduction operator</li><li><code>dst</code>: Output vector</li><li><code>src</code>: Input matrix of size <code>(n, p)</code></li><li><code>dim</code>: Dimension to reduce along (1 or 2)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p>For <code>dim=1</code> (column-wise):</p><ul><li><code>Nitem=nothing</code>: Items per thread</li><li><code>Nthreads=nothing</code>: Threads per column reduction</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks=nothing</code>: Number of blocks</li></ul><p>For <code>dim=2</code> (row-wise):</p><ul><li><code>chunksz=nothing</code>: Chunk size for row processing</li><li><code>Nblocks=nothing</code>: Number of blocks per row</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks_row=nothing</code>: Blocks per row</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)
col_sums = CUDA.zeros(Float32, 500)
row_maxs = CUDA.zeros(Float32, 1000)

# Column sums
mapreduce2d!(identity, +, col_sums, A, 1)

# Row maximums
mapreduce2d!(identity, max, row_maxs, A, 2)</code></pre><p>See also: <a href="#KernelForge.mapreduce2d"><code>KernelForge.mapreduce2d</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/2D/mapreduce2d.jl#L69-L115">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce1d"><a class="docstring-binding" href="#KernelForge.mapreduce1d"><code>KernelForge.mapreduce1d</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce1d(f, op, src; kwargs...) -&gt; GPU array or scalar
mapreduce1d(f, op, srcs::NTuple; kwargs...) -&gt; GPU array or scalar</code></pre><p>GPU parallel map-reduce operation.</p><p>Applies <code>f</code> to each element, reduces with <code>op</code>, and optionally applies <code>g</code> to the final result.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>src</code> or <code>srcs</code>: Input GPU array(s)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation applied to final result</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li><li><code>to_cpu=false</code>: If true, return scalar; otherwise return 1-element GPU array</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Sum of squares (returns GPU array)
x = CUDA.rand(Float32, 10_000)
result = mapreduce1d(x -&gt; x^2, +, x)

# Sum of squares (returns scalar)
result = mapreduce1d(x -&gt; x^2, +, x; to_cpu=true)

# Dot product of two arrays
x, y = CUDA.rand(Float32, 10_000), CUDA.rand(Float32, 10_000)
result = mapreduce1d((a, b) -&gt; a * b, +, (x, y); to_cpu=true)</code></pre><p>See also: <a href="#KernelForge.mapreduce1d!"><code>KernelForge.mapreduce1d!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/1D/mapreduce1d.jl#L1-L38">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce1d!"><a class="docstring-binding" href="#KernelForge.mapreduce1d!"><code>KernelForge.mapreduce1d!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce1d!(f, op, dst, src; kwargs...)
mapreduce1d!(f, op, dst, srcs::NTuple; kwargs...)</code></pre><p>In-place GPU parallel map-reduce, writing result to <code>dst[1]</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator  </li><li><code>dst</code>: Output array (result written to first element)</li><li><code>src</code> or <code>srcs</code>: Input GPU array(s)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation applied to final result</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = CUDA.zeros(Float32, 1)

# Sum
mapreduce1d!(identity, +, dst, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(mapreduce1d!, x)
for i in 1:100
    mapreduce1d!(identity, +, dst, x; tmp)
end</code></pre><p>See also: <a href="#KernelForge.mapreduce1d"><code>KernelForge.mapreduce1d</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/1D/mapreduce1d.jl#L41-L77">source</a></section></details></article><h2 id="Scan"><a class="docs-heading-anchor" href="#Scan">Scan</a><a id="Scan-1"></a><a class="docs-heading-anchor-permalink" href="#Scan" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.scan"><a class="docstring-binding" href="#KernelForge.scan"><code>KernelForge.scan</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">scan(f, op, src; kwargs...) -&gt; GPU array
scan(op, src; kwargs...) -&gt; GPU array</code></pre><p>GPU parallel prefix scan (cumulative reduction) using a decoupled lookback algorithm.</p><p>Applies <code>f</code> to each element, then computes inclusive prefix scan with <code>op</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element (defaults to <code>identity</code>)</li><li><code>op</code>: Associative binary scan operator</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Cumulative sum
x = CUDA.rand(Float32, 10_000)
result = scan(+, x)

# Cumulative sum of squares
result = scan(x -&gt; x^2, +, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(scan!, similar(x), x)
result = scan(+, x; tmp)</code></pre><p>See also: <a href="#KernelForge.scan!"><code>KernelForge.scan!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/scan/scan.jl#L1-L35">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.scan!"><a class="docstring-binding" href="#KernelForge.scan!"><code>KernelForge.scan!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">scan!(f, op, dst, src; kwargs...)
scan!(op, dst, src; kwargs...)</code></pre><p>In-place GPU parallel prefix scan using a decoupled lookback algorithm.</p><p>Applies <code>f</code> to each element, then computes inclusive prefix scan with <code>op</code>, writing results to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element (defaults to <code>identity</code>)</li><li><code>op</code>: Associative binary scan operator</li><li><code>dst</code>: Output array for scan results</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = similar(x)

# Cumulative sum
scan!(+, dst, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(scan!, dst, x)
for i in 1:100
    scan!(+, dst, x; tmp)
end</code></pre><p>See also: <a href="#KernelForge.scan"><code>KernelForge.scan</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/scan/scan.jl#L38-L75">source</a></section></details></article><h2 id="Matrix-Vector"><a class="docs-heading-anchor" href="#Matrix-Vector">Matrix-Vector</a><a id="Matrix-Vector-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Vector" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.matvec"><a class="docstring-binding" href="#KernelForge.matvec"><code>KernelForge.matvec</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">matvec([f, op,] src::AbstractMatrix, x; kwargs...) -&gt; dst
matvec!([f, op,] dst, src, x; kwargs...)</code></pre><p>Generalized matrix-vector operation with customizable element-wise and reduction operations.</p><p>Computes <code>dst[i] = g(op_j(f(src[i,j], x[j])))</code> for each row <code>i</code>, where <code>op_j</code> denotes reduction over columns. For standard matrix-vector multiplication, this is <code>dst[i] = sum_j(src[i,j] * x[j])</code>.</p><p>The allocating version <code>matvec</code> returns a newly allocated result vector. The in-place version <code>matvec!</code> writes to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Binary operation applied element-wise (default: <code>*</code>)</li><li><code>op</code>: Reduction operation across columns (default: <code>+</code>)</li><li><code>dst</code>: Output vector (in-place versions only)</li><li><code>src</code>: Input matrix</li><li><code>x</code>: Input vector, or <code>nothing</code> for row-wise reduction of <code>src</code> alone</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Unary transformation applied to each reduced row</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer for inter-block communication</li><li><code>chunksz=nothing</code>: Elements per thread (auto-tuned if <code>nothing</code>)</li><li><code>Nblocks=nothing</code>: Number of thread blocks (auto-tuned if <code>nothing</code>)</li><li><code>workgroup=nothing</code>: Threads per block (auto-tuned if <code>nothing</code>)</li><li><code>blocks_row=nothing</code>: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if <code>nothing</code>.</li><li><code>FlagType=UInt8</code>: Integer type for synchronization flags</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)
x = CUDA.rand(Float32, 500)

# Standard matrix-vector multiply: y = A * x
y = matvec(A, x)

# Row-wise sum: y[i] = sum(A[i, :])
y = matvec(A, nothing)

# Row-wise maximum: y[i] = max_j(A[i, j])
y = matvec(identity, max, A, nothing)

# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))
y = matvec((a, b) -&gt; exp(a - b), +, A, x)

# In-place version
dst = CUDA.zeros(Float32, 1000)
matvec!(dst, A, x)</code></pre><p><strong>Extended Help</strong></p><p>For tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from <code>blocks_row</code>. <code>blocks_row</code> is equal to  Nblocks for a large row matrix.</p><p>Pre-allocating <code>tmp</code> avoids repeated allocation when calling <code>matvec!</code> in a loop. With <code>FlagType=UInt8</code> (default), the flag buffer must be zeroed before each call. Using <code>FlagType=UInt64</code> skips this zeroing by generating a random target flag at each call; correctness holds with probability <code>1 - n/2^64</code>, which is negligible for practical <code>n</code>. Output element type is inferred as <code>promote_op(g, promote_op(f, eltype(src), eltype(x)))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/2D/matvec.jl#L1-L66">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.matvec!"><a class="docstring-binding" href="#KernelForge.matvec!"><code>KernelForge.matvec!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">matvec([f, op,] src::AbstractMatrix, x; kwargs...) -&gt; dst
matvec!([f, op,] dst, src, x; kwargs...)</code></pre><p>Generalized matrix-vector operation with customizable element-wise and reduction operations.</p><p>Computes <code>dst[i] = g(op_j(f(src[i,j], x[j])))</code> for each row <code>i</code>, where <code>op_j</code> denotes reduction over columns. For standard matrix-vector multiplication, this is <code>dst[i] = sum_j(src[i,j] * x[j])</code>.</p><p>The allocating version <code>matvec</code> returns a newly allocated result vector. The in-place version <code>matvec!</code> writes to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Binary operation applied element-wise (default: <code>*</code>)</li><li><code>op</code>: Reduction operation across columns (default: <code>+</code>)</li><li><code>dst</code>: Output vector (in-place versions only)</li><li><code>src</code>: Input matrix</li><li><code>x</code>: Input vector, or <code>nothing</code> for row-wise reduction of <code>src</code> alone</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Unary transformation applied to each reduced row</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer for inter-block communication</li><li><code>chunksz=nothing</code>: Elements per thread (auto-tuned if <code>nothing</code>)</li><li><code>Nblocks=nothing</code>: Number of thread blocks (auto-tuned if <code>nothing</code>)</li><li><code>workgroup=nothing</code>: Threads per block (auto-tuned if <code>nothing</code>)</li><li><code>blocks_row=nothing</code>: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if <code>nothing</code>.</li><li><code>FlagType=UInt8</code>: Integer type for synchronization flags</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)
x = CUDA.rand(Float32, 500)

# Standard matrix-vector multiply: y = A * x
y = matvec(A, x)

# Row-wise sum: y[i] = sum(A[i, :])
y = matvec(A, nothing)

# Row-wise maximum: y[i] = max_j(A[i, j])
y = matvec(identity, max, A, nothing)

# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))
y = matvec((a, b) -&gt; exp(a - b), +, A, x)

# In-place version
dst = CUDA.zeros(Float32, 1000)
matvec!(dst, A, x)</code></pre><p><strong>Extended Help</strong></p><p>For tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from <code>blocks_row</code>. <code>blocks_row</code> is equal to  Nblocks for a large row matrix.</p><p>Pre-allocating <code>tmp</code> avoids repeated allocation when calling <code>matvec!</code> in a loop. With <code>FlagType=UInt8</code> (default), the flag buffer must be zeroed before each call. Using <code>FlagType=UInt64</code> skips this zeroing by generating a random target flag at each call; correctness holds with probability <code>1 - n/2^64</code>, which is negligible for practical <code>n</code>. Output element type is inferred as <code>promote_op(g, promote_op(f, eltype(src), eltype(x)))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/2D/matvec.jl#L1-L66">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.vecmat!"><a class="docstring-binding" href="#KernelForge.vecmat!"><code>KernelForge.vecmat!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">vecmat!(dst, x, A; kwargs...)
vecmat!(f, op, dst, x, A; kwargs...)</code></pre><p>GPU parallel vector-matrix multiplication: <code>dst = g(op(f(x .* A), dims=1))</code>.</p><p>For standard matrix-vector product: <code>vecmat!(dst, x, A)</code> computes <code>dst[j] = sum(x[i] * A[i,j])</code>. When <code>x = nothing</code>, computes column reductions: <code>dst[j] = sum(A[i,j])</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f=identity</code>: Element-wise transformation applied to <code>x[i] * A[i,j]</code> (or <code>A[i,j]</code> if <code>x=nothing</code>)</li><li><code>op=+</code>: Reduction operator</li><li><code>dst</code>: Output vector of length <code>p</code> (number of columns)</li><li><code>x</code>: Input vector of length <code>n</code> (number of rows), or <code>nothing</code> for pure column reduction</li><li><code>A</code>: Input matrix of size <code>(n, p)</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Optional post-reduction transformation</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer (from <code>get_allocation</code>)</li><li><code>Nitem=nothing</code>: Number of items per thread (auto-selected if nothing)</li><li><code>Nthreads=nothing</code>: Number of threads per column reduction</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks=nothing</code>: Maximum number of blocks</li><li><code>FlagType=UInt8</code>: Type for synchronization flags</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/2D/vecmat.jl#L1-L25">source</a></section></details></article><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.get_allocation"><a class="docstring-binding" href="#KernelForge.get_allocation"><code>KernelForge.get_allocation</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">get_allocation(::typeof(mapreduce1d!), src; blocks=100, eltype=nothing, FlagType=UInt8)</code></pre><p>Allocate temporary buffer for <code>mapreduce1d!</code>. Useful for repeated reductions.</p><p><strong>Arguments</strong></p><ul><li><code>src</code> or <code>srcs</code>: Input GPU array(s) (used for backend and default element type)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>blocks=100</code>: Number of blocks (must match the <code>blocks</code> used in <code>mapreduce1d!</code>)</li><li><code>eltype=nothing</code>: Element type for intermediate values. If <code>nothing</code>, defaults to  the element type of <code>src</code>. For proper type inference, pass <code>promote_op(f, T, ...)</code>.</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
tmp = KernelForge.get_allocation(mapreduce1d!, x)
dst = CUDA.zeros(Float32, 1)

for i in 1:100
    mapreduce1d!(identity, +, dst, x; tmp)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/mapreduce/1D/mapreduce1d.jl#L100-L124">source</a></section><section><div><pre><code class="language-julia hljs">get_allocation(::typeof(scan!), dst, src; kwargs...)</code></pre><p>Allocate temporary buffer for <code>scan!</code>. Useful for repeated scans.</p><p><strong>Arguments</strong></p><ul><li><code>dst</code>: Output GPU array (used for element type of intermediates)</li><li><code>src</code>: Input GPU array (used for backend)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size (must match the <code>workgroup</code> used in <code>scan!</code>)</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = similar(x)
tmp = KernelForge.get_allocation(scan!, dst, x)

for i in 1:100
    scan!(+, dst, x; tmp)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/6911b56f9e10cae5f59fbd4d15b9f79a6a7d9636/src/scan/scan.jl#L109-L133">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 4 February 2026 21:07">Wednesday 4 February 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
