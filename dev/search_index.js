var documenterSearchIndex = {"docs":
[{"location":"performances/#Performance","page":"Performance","title":"Performance","text":"KernelForge.jl achieves performance comparable to optimized CUDA C++ libraries such as CUB. Benchmarks report two metrics:\n\nKernel time: Execution time of the main kernel, measured using @profile from CUDA.jl\nOverhead: Total time minus kernel time, including memory allocations and data transfers","category":"section"},{"location":"performances/#Copy","page":"Performance","title":"Copy","text":"CUDA.jl leverages the proprietary libcuda library for memory copies, which internally vectorizes loads and stores. In contrast, the cross-platform GPUArrayCore.jl relies on KernelAbstractions.jl, which does not currently perform vectorization. KernelForge's vcopy! bridges this gap by using vload and vstore operations built on unsafe pointer access via LLVMPtrs from KernelIntrinsics.jl.\n\nThe graph below compares memory bandwidth for Float32 and UInt8 data types. With vectorized loads and stores, KernelForge achieves bandwidth comparable to CUDA.jl for both types. The slight underperformance below the L2 cache threshold stems from our current vectorization factor (×8 for Float32); increasing this to ×16 would close the remaining gap.\n\n(Image: Copy Bandwidth)","category":"section"},{"location":"performances/#Map-Reduce","page":"Performance","title":"Map-Reduce","text":"KernelForge.jl matches CUDA.jl performance on Float32 and significantly outperforms it on smaller types (UInt8, UnitFloat8), even when converting to Float32 during reduction. These gains result from optimized memory access patterns and vectorized loads/stores.\n\n(Image: Map-Reduce Benchmark)","category":"section"},{"location":"performances/#Scan","page":"Performance","title":"Scan","text":"KernelForge's scan kernel rivals CUB performance on Float32 and Float64, while additionally supporting non-commutative operations and custom types such as Quaternions. This is achieved through an efficient decoupled lookback algorithm combined with optimized memory access.\n\n(Image: Scan Benchmark)","category":"section"},{"location":"performances/#Matrix-Vector-Operations","page":"Performance","title":"Matrix-Vector Operations","text":"KernelForge implements matrix-vector and vector-matrix operations for general types and operators. For benchmarking, we compare against CUDA.jl on Float32, which internally calls cuBLAS's gemv routine.\n\nDue to column-major memory layout, matrix-vector and vector-matrix multiplications have fundamentally different access patterns. KernelForge therefore provides separate optimized kernels for each operation.\n\nFor both benchmarks, we fix the total matrix size (n × p) and vary n from 10 to (n × p) / 10, sweeping from tall-narrow to short-wide matrices. The black line indicates the reduced overhead achieved when the user provides pre-allocated temporary memory.\n\nMatrix-Vector (Image: Matrix-Vector Benchmark)\n\nVector-Matrix\n\n(Image: Vector-Matrix Benchmark)","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Copy","page":"API Reference","title":"Copy","text":"","category":"section"},{"location":"api/#Map-Reduce","page":"API Reference","title":"Map-Reduce","text":"","category":"section"},{"location":"api/#Scan","page":"API Reference","title":"Scan","text":"","category":"section"},{"location":"api/#Matrix-Vector","page":"API Reference","title":"Matrix-Vector","text":"","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/#KernelForge.vcopy!","page":"API Reference","title":"KernelForge.vcopy!","text":"vcopy!(dst::AbstractGPUVector, src::AbstractGPUVector; Nitem=4)\n\nCopy src to dst using vectorized GPU memory access.\n\nPerforms a high-throughput copy by loading and storing Nitem elements per thread, reducing memory transaction overhead compared to scalar copies.\n\nArguments\n\ndst: Destination GPU vector\nsrc: Source GPU vector (must have same length as dst)\nNitem=4: Number of elements processed per thread. Higher values improve throughput but require length(src) to be divisible by Nitem.\n\nExample\n\nsrc = CUDA.rand(Float32, 1024)\ndst = CUDA.zeros(Float32, 1024)\nvcopy!(dst, src)\n\nSee also: KernelForge.setvalue!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.setvalue!","page":"API Reference","title":"KernelForge.setvalue!","text":"setvalue!(dst::AbstractGPUVector{T}, val::T; Nitem=4) where T\n\nFill dst with val using vectorized GPU memory access.\n\nPerforms a high-throughput fill by storing Nitem copies of val per thread, reducing memory transaction overhead compared to scalar writes.\n\nArguments\n\ndst: Destination GPU vector\nval: Value to fill (must match element type of dst)\nNitem=4: Number of elements written per thread. Higher values improve throughput but require length(dst) to be divisible by Nitem.\n\nExample\n\ndst = CUDA.zeros(Float32, 1024)\nsetvalue!(dst, 1.0f0)\n\nSee also: KernelForge.vcopy!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce","page":"API Reference","title":"KernelForge.mapreduce","text":"mapreduce(f, op, src::AbstractGPUArray; dims=nothing, kwargs...) -> Array or scalar\n\nGPU parallel map-reduce operation with optional dimension reduction.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\nsrc: Input GPU array\n\nKeyword Arguments\n\ndims=nothing: Dimensions to reduce over. Options:\nnothing or :: Reduce over all dimensions (returns scalar or 1-element array)\nInt: Reduce over single dimension\nTuple{Int...}: Reduce over multiple dimensions (must be contiguous from start or end)\ng=identity: Post-reduction transformation\ninit=nothing: Initial value (currently unused, for API compatibility)\nto_cpu=false: If true and dims=nothing, return scalar; otherwise return GPU array\nAdditional kwargs passed to underlying implementations\n\nDimension Constraints\n\nThe dims argument must specify contiguous dimensions from either:\n\nThe beginning: (1,), (1,2), (1,2,3), etc.\nThe end: (n-1,n), (n,), etc. for an n-dimensional array\n\nExamples\n\nA = CUDA.rand(Float32, 100, 50, 20)\n\n# Full reduction (all dimensions)\ntotal = mapreduce(identity, +, A; to_cpu=true)\n\n# Reduce along dim 1: (100, 50, 20) -> (50, 20)\ncol_sums = mapreduce(identity, +, A; dims=1)\n\n# Reduce along dims (1,2): (100, 50, 20) -> (20,)\nplane_sums = mapreduce(identity, +, A; dims=(1,2))\n\n# Reduce along last dim: (100, 50, 20) -> (100, 50)\ndepth_sums = mapreduce(identity, +, A; dims=3)\n\n# Reduce along last two dims: (100, 50, 20) -> (100,)\nslice_sums = mapreduce(identity, +, A; dims=(2,3))\n\nSee also: KernelForge.mapreduce!, mapreduce1d, mapreduce2d\n\n\n\n\n\nmapreduce(f, op, srcs::NTuple{N,AbstractGPUArray}; dims=nothing, kwargs...)\n\nMulti-array mapreduce. Only supports full reduction (dims=nothing).\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce!","page":"API Reference","title":"KernelForge.mapreduce!","text":"mapreduce!(f, op, dst, src; dims=nothing, kwargs...)\n\nIn-place GPU parallel map-reduce with dimension support.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\ndst: Output array\nsrc: Input GPU array\n\nKeyword Arguments\n\ndims=nothing: Dimensions to reduce over (see mapreduce for details)\ng=identity: Post-reduction transformation\nAdditional kwargs passed to underlying implementations\n\nExamples\n\nA = CUDA.rand(Float32, 100, 50)\ncol_sums = CUDA.zeros(Float32, 50)\nrow_sums = CUDA.zeros(Float32, 100)\n\n# Column sums (reduce dim 1)\nmapreduce!(identity, +, col_sums, A; dims=1)\n\n# Row sums (reduce dim 2)\nmapreduce!(identity, +, row_sums, A; dims=2)\n\nSee also: KernelForge.mapreduce\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce2d","page":"API Reference","title":"KernelForge.mapreduce2d","text":"mapreduce2d(f, op, src, dim; kwargs...) -> Vector\n\nGPU parallel reduction along dimension dim.\n\ndim=1: Column-wise reduction (vertical), output length = number of columns\ndim=2: Row-wise reduction (horizontal), output length = number of rows\n\nArguments\n\nf: Element-wise transformation\nop: Reduction operator\nsrc: Input matrix of size (n, p)\ndim: Dimension to reduce along (1 or 2)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation\ntmp=nothing: Pre-allocated temporary buffer\nFlagType=UInt8: Synchronization flag type\n\nFor dim=1 (column-wise):\n\nNitem=nothing: Items per thread\nNthreads=nothing: Threads per column reduction\nworkgroup=nothing: Workgroup size\nblocks=nothing: Number of blocks\n\nFor dim=2 (row-wise):\n\nchunksz=nothing: Chunk size for row processing\nNblocks=nothing: Number of blocks per row\nworkgroup=nothing: Workgroup size\nblocks_row=nothing: Blocks per row\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Column sums (reduce along dim=1)\ncol_sums = mapreduce2d(identity, +, A, 1)\n\n# Row maximums (reduce along dim=2)\nrow_maxs = mapreduce2d(identity, max, A, 2)\n\n# Column means\ncol_means = mapreduce2d(identity, +, A, 1; g=x -> x / size(A, 1))\n\n# Sum of squares per row\nrow_ss = mapreduce2d(abs2, +, A, 2)\n\nSee also: KernelForge.mapreduce2d! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce2d!","page":"API Reference","title":"KernelForge.mapreduce2d!","text":"mapreduce2d!(f, op, dst, src, dim; kwargs...)\n\nIn-place GPU parallel reduction along dimension dim.\n\ndim=1: Column-wise reduction (vertical), dst length = number of columns\ndim=2: Row-wise reduction (horizontal), dst length = number of rows\n\nArguments\n\nf: Element-wise transformation\nop: Reduction operator\ndst: Output vector\nsrc: Input matrix of size (n, p)\ndim: Dimension to reduce along (1 or 2)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation\ntmp=nothing: Pre-allocated temporary buffer\nFlagType=UInt8: Synchronization flag type\n\nFor dim=1 (column-wise):\n\nNitem=nothing: Items per thread\nNthreads=nothing: Threads per column reduction\nworkgroup=nothing: Workgroup size\nblocks=nothing: Number of blocks\n\nFor dim=2 (row-wise):\n\nchunksz=nothing: Chunk size for row processing\nNblocks=nothing: Number of blocks per row\nworkgroup=nothing: Workgroup size\nblocks_row=nothing: Blocks per row\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\ncol_sums = CUDA.zeros(Float32, 500)\nrow_maxs = CUDA.zeros(Float32, 1000)\n\n# Column sums\nmapreduce2d!(identity, +, col_sums, A, 1)\n\n# Row maximums\nmapreduce2d!(identity, max, row_maxs, A, 2)\n\nSee also: KernelForge.mapreduce2d for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce1d","page":"API Reference","title":"KernelForge.mapreduce1d","text":"mapreduce1d(f, op, src; kwargs...) -> GPU array or scalar\nmapreduce1d(f, op, srcs::NTuple; kwargs...) -> GPU array or scalar\n\nGPU parallel map-reduce operation.\n\nApplies f to each element, reduces with op, and optionally applies g to the final result.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\nsrc or srcs: Input GPU array(s)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation applied to final result\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\nFlagType=UInt8: Synchronization flag type\nto_cpu=false: If true, return scalar; otherwise return 1-element GPU array\n\nExamples\n\n# Sum of squares (returns GPU array)\nx = CUDA.rand(Float32, 10_000)\nresult = mapreduce1d(x -> x^2, +, x)\n\n# Sum of squares (returns scalar)\nresult = mapreduce1d(x -> x^2, +, x; to_cpu=true)\n\n# Dot product of two arrays\nx, y = CUDA.rand(Float32, 10_000), CUDA.rand(Float32, 10_000)\nresult = mapreduce1d((a, b) -> a * b, +, (x, y); to_cpu=true)\n\nSee also: KernelForge.mapreduce1d! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce1d!","page":"API Reference","title":"KernelForge.mapreduce1d!","text":"mapreduce1d!(f, op, dst, src; kwargs...)\nmapreduce1d!(f, op, dst, srcs::NTuple; kwargs...)\n\nIn-place GPU parallel map-reduce, writing result to dst[1].\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator  \ndst: Output array (result written to first element)\nsrc or srcs: Input GPU array(s)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation applied to final result\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = CUDA.zeros(Float32, 1)\n\n# Sum\nmapreduce1d!(identity, +, dst, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(mapreduce1d!, x)\nfor i in 1:100\n    mapreduce1d!(identity, +, dst, x; tmp)\nend\n\nSee also: KernelForge.mapreduce1d for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.scan","page":"API Reference","title":"KernelForge.scan","text":"scan(f, op, src; kwargs...) -> GPU array\nscan(op, src; kwargs...) -> GPU array\n\nGPU parallel prefix scan (cumulative reduction) using a decoupled lookback algorithm.\n\nApplies f to each element, then computes inclusive prefix scan with op.\n\nArguments\n\nf: Map function applied to each element (defaults to identity)\nop: Associative binary scan operator\nsrc: Input GPU array\n\nKeyword Arguments\n\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\n# Cumulative sum\nx = CUDA.rand(Float32, 10_000)\nresult = scan(+, x)\n\n# Cumulative sum of squares\nresult = scan(x -> x^2, +, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(scan!, similar(x), x)\nresult = scan(+, x; tmp)\n\nSee also: KernelForge.scan! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.scan!","page":"API Reference","title":"KernelForge.scan!","text":"scan!(f, op, dst, src; kwargs...)\nscan!(op, dst, src; kwargs...)\n\nIn-place GPU parallel prefix scan using a decoupled lookback algorithm.\n\nApplies f to each element, then computes inclusive prefix scan with op, writing results to dst.\n\nArguments\n\nf: Map function applied to each element (defaults to identity)\nop: Associative binary scan operator\ndst: Output array for scan results\nsrc: Input GPU array\n\nKeyword Arguments\n\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = similar(x)\n\n# Cumulative sum\nscan!(+, dst, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(scan!, dst, x)\nfor i in 1:100\n    scan!(+, dst, x; tmp)\nend\n\nSee also: KernelForge.scan for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.matvec","page":"API Reference","title":"KernelForge.matvec","text":"matvec([f, op,] src::AbstractMatrix, x; kwargs...) -> dst\nmatvec!([f, op,] dst, src, x; kwargs...)\n\nGeneralized matrix-vector operation with customizable element-wise and reduction operations.\n\nComputes dst[i] = g(op_j(f(src[i,j], x[j]))) for each row i, where op_j denotes reduction over columns. For standard matrix-vector multiplication, this is dst[i] = sum_j(src[i,j] * x[j]).\n\nThe allocating version matvec returns a newly allocated result vector. The in-place version matvec! writes to dst.\n\nArguments\n\nf: Binary operation applied element-wise (default: *)\nop: Reduction operation across columns (default: +)\ndst: Output vector (in-place versions only)\nsrc: Input matrix\nx: Input vector, or nothing for row-wise reduction of src alone\n\nKeyword Arguments\n\ng=identity: Unary transformation applied to each reduced row\ntmp=nothing: Pre-allocated temporary buffer for inter-block communication\nchunksz=nothing: Elements per thread (auto-tuned if nothing)\nNblocks=nothing: Number of thread blocks (auto-tuned if nothing)\nworkgroup=nothing: Threads per block (auto-tuned if nothing)\nblocks_row=nothing: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if nothing.\nFlagType=UInt8: Integer type for synchronization flags\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# Standard matrix-vector multiply: y = A * x\ny = matvec(A, x)\n\n# Row-wise sum: y[i] = sum(A[i, :])\ny = matvec(A, nothing)\n\n# Row-wise maximum: y[i] = max_j(A[i, j])\ny = matvec(identity, max, A, nothing)\n\n# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))\ny = matvec((a, b) -> exp(a - b), +, A, x)\n\n# In-place version\ndst = CUDA.zeros(Float32, 1000)\nmatvec!(dst, A, x)\n\nExtended Help\n\nFor tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from blocks_row. blocks_row is equal to  Nblocks for a large row matrix.\n\nPre-allocating tmp avoids repeated allocation when calling matvec! in a loop. With FlagType=UInt8 (default), the flag buffer must be zeroed before each call. Using FlagType=UInt64 skips this zeroing by generating a random target flag at each call; correctness holds with probability 1 - n/2^64, which is negligible for practical n. Output element type is inferred as promote_op(g, promote_op(f, eltype(src), eltype(x))).\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.matvec!","page":"API Reference","title":"KernelForge.matvec!","text":"matvec([f, op,] src::AbstractMatrix, x; kwargs...) -> dst\nmatvec!([f, op,] dst, src, x; kwargs...)\n\nGeneralized matrix-vector operation with customizable element-wise and reduction operations.\n\nComputes dst[i] = g(op_j(f(src[i,j], x[j]))) for each row i, where op_j denotes reduction over columns. For standard matrix-vector multiplication, this is dst[i] = sum_j(src[i,j] * x[j]).\n\nThe allocating version matvec returns a newly allocated result vector. The in-place version matvec! writes to dst.\n\nArguments\n\nf: Binary operation applied element-wise (default: *)\nop: Reduction operation across columns (default: +)\ndst: Output vector (in-place versions only)\nsrc: Input matrix\nx: Input vector, or nothing for row-wise reduction of src alone\n\nKeyword Arguments\n\ng=identity: Unary transformation applied to each reduced row\ntmp=nothing: Pre-allocated temporary buffer for inter-block communication\nchunksz=nothing: Elements per thread (auto-tuned if nothing)\nNblocks=nothing: Number of thread blocks (auto-tuned if nothing)\nworkgroup=nothing: Threads per block (auto-tuned if nothing)\nblocks_row=nothing: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if nothing.\nFlagType=UInt8: Integer type for synchronization flags\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# Standard matrix-vector multiply: y = A * x\ny = matvec(A, x)\n\n# Row-wise sum: y[i] = sum(A[i, :])\ny = matvec(A, nothing)\n\n# Row-wise maximum: y[i] = max_j(A[i, j])\ny = matvec(identity, max, A, nothing)\n\n# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))\ny = matvec((a, b) -> exp(a - b), +, A, x)\n\n# In-place version\ndst = CUDA.zeros(Float32, 1000)\nmatvec!(dst, A, x)\n\nExtended Help\n\nFor tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from blocks_row. blocks_row is equal to  Nblocks for a large row matrix.\n\nPre-allocating tmp avoids repeated allocation when calling matvec! in a loop. With FlagType=UInt8 (default), the flag buffer must be zeroed before each call. Using FlagType=UInt64 skips this zeroing by generating a random target flag at each call; correctness holds with probability 1 - n/2^64, which is negligible for practical n. Output element type is inferred as promote_op(g, promote_op(f, eltype(src), eltype(x))).\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.vecmat!","page":"API Reference","title":"KernelForge.vecmat!","text":"vecmat!(dst, x, A; kwargs...)\nvecmat!(f, op, dst, x, A; kwargs...)\n\nGPU parallel vector-matrix multiplication: dst = g(op(f(x .* A), dims=1)).\n\nFor standard matrix-vector product: vecmat!(dst, x, A) computes dst[j] = sum(x[i] * A[i,j]). When x = nothing, computes column reductions: dst[j] = sum(A[i,j]).\n\nArguments\n\nf=identity: Element-wise transformation applied to x[i] * A[i,j] (or A[i,j] if x=nothing)\nop=+: Reduction operator\ndst: Output vector of length p (number of columns)\nx: Input vector of length n (number of rows), or nothing for pure column reduction\nA: Input matrix of size (n, p)\n\nKeyword Arguments\n\ng=identity: Optional post-reduction transformation\ntmp=nothing: Pre-allocated temporary buffer (from get_allocation)\nNitem=nothing: Number of items per thread (auto-selected if nothing)\nNthreads=nothing: Number of threads per column reduction\nworkgroup=nothing: Workgroup size\nblocks=nothing: Maximum number of blocks\nFlagType=UInt8: Type for synchronization flags\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.get_allocation","page":"API Reference","title":"KernelForge.get_allocation","text":"get_allocation(::typeof(mapreduce1d!), src; blocks=100, eltype=nothing, FlagType=UInt8)\n\nAllocate temporary buffer for mapreduce1d!. Useful for repeated reductions.\n\nArguments\n\nsrc or srcs: Input GPU array(s) (used for backend and default element type)\n\nKeyword Arguments\n\nblocks=100: Number of blocks (must match the blocks used in mapreduce1d!)\neltype=nothing: Element type for intermediate values. If nothing, defaults to  the element type of src. For proper type inference, pass promote_op(f, T, ...).\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ntmp = KernelForge.get_allocation(mapreduce1d!, x)\ndst = CUDA.zeros(Float32, 1)\n\nfor i in 1:100\n    mapreduce1d!(identity, +, dst, x; tmp)\nend\n\n\n\n\n\nget_allocation(::typeof(scan!), dst, src; kwargs...)\n\nAllocate temporary buffer for scan!. Useful for repeated scans.\n\nArguments\n\ndst: Output GPU array (used for element type of intermediates)\nsrc: Input GPU array (used for backend)\n\nKeyword Arguments\n\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size (must match the workgroup used in scan!)\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = similar(x)\ntmp = KernelForge.get_allocation(scan!, dst, x)\n\nfor i in 1:100\n    scan!(+, dst, x; tmp)\nend\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides practical examples demonstrating KernelForge.jl's capabilities.","category":"section"},{"location":"examples/#Vectorized-Copy","page":"Examples","title":"Vectorized Copy","text":"Perform memory copies with vectorized loads and stores for improved bandwidth utilization:\n\nusing KernelForge\nusing CUDA\n\nsrc = CUDA.rand(Float32, 10^6)\ndst = similar(src)\n\n# Copy with vectorized loads and stores\nKernelForge.vcopy!(dst, src; Nitem=4)\n\n@assert dst ≈ src","category":"section"},{"location":"examples/#Custom-Types","page":"Examples","title":"Custom Types","text":"KernelForge supports copying custom struct types:\n\nusing KernelForge\nusing CUDA\n\nstruct Point3D\n    x::Float32\n    y::Float32\n    z::Float32\nend\n\nn = 100_000\nsrc = CuArray([Point3D(rand(), rand(), rand()) for _ in 1:n])\ndst = similar(src)\n\nKernelForge.vcopy!(dst, src; Nitem=2)\n\n@assert all(dst .== src)","category":"section"},{"location":"examples/#Set-Values","page":"Examples","title":"Set Values","text":"Fill an array with a constant value:\n\nusing KernelForge\nusing CUDA\n\ndst = CUDA.ones(UInt8, 100_000)\nKernelForge.setvalue!(dst, 0x00; Nitem=4)\n\n@assert all(dst .== 0x00)","category":"section"},{"location":"examples/#Map-Reduce","page":"Examples","title":"Map-Reduce","text":"","category":"section"},{"location":"examples/#Basic-Reductions","page":"Examples","title":"Basic Reductions","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 10^6)\n\n# Sum\ntotal = KernelForge.mapreduce(identity, +, x; to_cpu=true)\n\n# Sum of squares\nsum_sq = KernelForge.mapreduce(abs2, +, x; to_cpu=true)\n\n# Maximum\nmax_val = KernelForge.mapreduce(identity, max, x; to_cpu=true)\n\n# Minimum\nmin_val = KernelForge.mapreduce(identity, min, x; to_cpu=true)","category":"section"},{"location":"examples/#Post-Reduction-Transformation,-Example-with-KernelForge.UnitFloat8","page":"Examples","title":"Post-Reduction Transformation, Example with KernelForge.UnitFloat8","text":"Apply a function after reduction using the g parameter:\n\nusing KernelForge\nusing CUDA\n\nimport KernelForge: UnitFloat8\nx = CuArray{UnitFloat8}([rand(UnitFloat8) for _ in 1:10^6])\n\n# convert to float 32 for reduction to avoid overflow\nto_f32(x) = Float32(x)\n# then back to unitfloat8\nto_uif8(x) = UnitFloat8(x)\nsum_uif8 = KernelForge.mapreduce(to_f32, +, x; g=to_uif8, to_cpu=true)\n\n# compare the sign only :\n@assert sign(Float32(sum_uif8)) == sign(sum(Float32.(x)))\n\n# We can store only 8 bit, keep rather good precision (because of intermediate Float32 conversion) and get \n# same performance as for UInt8 addition !!!","category":"section"},{"location":"examples/#Dot-Product-and-Distance","page":"Examples","title":"Dot Product and Distance","text":"Reduce multiple arrays simultaneously:\n\nusing KernelForge\nusing CUDA\n\nn = 100_000\na = CUDA.rand(Float32, n)\nb = CUDA.rand(Float32, n)\n\n# Dot product: sum(a .* b)\ndot_prod = KernelForge.mapreduce1d((x, y) -> x * y, +, (a, b); to_cpu=true)\n\n# Euclidean distance: sqrt(sum((a - b)^2))\ndistance = KernelForge.mapreduce1d((x, y) -> (x - y)^2, +, (a, b); g=sqrt, to_cpu=true)\n\n# Weighted sum: sum(w .* x)\nw = CUDA.rand(Float32, n)\nx = CUDA.rand(Float32, n)\nweighted_sum = KernelForge.mapreduce1d((wi, xi) -> wi * xi, +, (w, x); to_cpu=true)","category":"section"},{"location":"examples/#2D-Reductions","page":"Examples","title":"2D Reductions","text":"Reduce along rows or columns:\n\nusing KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Column sums (reduce along dim=1)\ncol_sums = KernelForge.mapreduce(identity, +, A; dims=1)\n@assert size(col_sums) == (500,)\n\n# Row sums (reduce along dim=2)\nrow_sums = KernelForge.mapreduce(identity, +, A; dims=2)\n@assert size(row_sums) == (1000,)\n\n# Column means\nlet u = size(A, 1) # Note that the let block is necessary for compilation of the kernel\n    g(x)::Float32 = x / u\n    col_means = KernelForge.mapreduce(identity, +, A; dims=1, g=g)\nend\n# Row-wise sum of squares\nrow_ss = KernelForge.mapreduce(abs2, +, A; dims=2)\n\n# Row maximums\nrow_max = KernelForge.mapreduce(identity, max, A; dims=2)","category":"section"},{"location":"examples/#Higher-Dimensional-Reductions","page":"Examples","title":"Higher-Dimensional Reductions","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 20, 30, 40)\n\n\n# !!! For now, we provide support only for contiguous dim at start or end, so we cannot reduce on second dim here\ntry\n    KernelForge.mapreduce(identity, +, A; dims=2)\ncatch e\n    println(e)\nend\n\n# Reduce first dimension: (20, 30, 40) → (30, 40)\nresult = KernelForge.mapreduce(identity, +, A; dims=1)\n@assert size(result) == (30, 40)\n\n# Reduce first two dimensions: (20, 30, 40) → (40,)\nresult = KernelForge.mapreduce(identity, +, A; dims=(1, 2))\n@assert size(result) == (40,)\n\n# Reduce last dimension: (20, 30, 40) → (20, 30)\nresult = KernelForge.mapreduce(identity, +, A; dims=3)\n@assert size(result) == (20, 30)\n\n# Reduce last two dimensions: (20, 30, 40) → (20,)\nresult = KernelForge.mapreduce(identity, +, A; dims=(2, 3))\n@assert size(result) == (20,)","category":"section"},{"location":"examples/#Custom-Structs","page":"Examples","title":"Custom Structs","text":"using KernelForge\nusing CUDA\n\nstruct Stats\n    sum::Float32\n    sum_sq::Float32\n    count::Float32\nend\n\n# Map: convert each value to Stats\nf(x) = Stats(x, x^2, 1f0)\n\n# Reduce: combine Stats\nop(a::Stats, b::Stats) = Stats(a.sum + b.sum, a.sum_sq + b.sum_sq, a.count + b.count)\n\nx = CUDA.rand(Float32, 100_000)\nresult = KernelForge.mapreduce(f, op, x; to_cpu=true)\n\nmean = result.sum / result.count\nvariance = result.sum_sq / result.count - mean^2","category":"section"},{"location":"examples/#Prefix-Scan","page":"Examples","title":"Prefix Scan","text":"","category":"section"},{"location":"examples/#Basic-Scans","page":"Examples","title":"Basic Scans","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 10^6)\ndst = similar(x)\n\n# Cumulative sum\nKernelForge.scan!(+, dst, x)\n@assert dst ≈ CuArray(accumulate(+, Array(x)))\n\n# Cumulative product\nKernelForge.scan!(*, dst, x)\n@assert dst ≈ CuArray(accumulate(*, Array(x)))\n\n# Cumulative maximum\nKernelForge.scan!(max, dst, x)\n@assert dst ≈ CuArray(accumulate(max, Array(x)))","category":"section"},{"location":"examples/#Scan-with-Map-Function","page":"Examples","title":"Scan with Map Function","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 10_000)\n\n# Cumulative sum of squares\nresult = KernelForge.scan(abs2, +, x)\n@assert result ≈ CuArray(accumulate(+, Array(x).^2))\n\n# Cumulative sum of absolute values\nresult = KernelForge.scan(abs, +, x)","category":"section"},{"location":"examples/#Non-Commutative-Operations:-Quaternions","page":"Examples","title":"Non-Commutative Operations: Quaternions","text":"KernelForge scan function (not mapreduce) correctly handles non-commutative operations:\n\nusing KernelForge\nusing CUDA\nusing Quaternions\n\nn = 100_000\n\n# Generate random unit quaternions\nsrc_cpu = [QuaternionF64((x ./ sqrt(sum(x.^2)))...) for x in eachcol(randn(4, n))]\nsrc = CuArray(src_cpu)\ndst = similar(src)\n\n# Quaternion multiplication is non-commutative: q1 * q2 ≠ q2 * q1\nKernelForge.scan!(*, dst, src)\n\n@assert dst ≈ CuArray(accumulate(*, src_cpu))","category":"section"},{"location":"examples/#Matrix-Vector-Operations","page":"Examples","title":"Matrix-Vector Operations","text":"","category":"section"},{"location":"examples/#Standard-Matrix-Vector-Multiply","page":"Examples","title":"Standard Matrix-Vector Multiply","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# y = A * x\ny = KernelForge.matvec(A, x)\n@assert y ≈ A * x\n\n# In-place version\ndst = CUDA.zeros(Float32, 1000)\nKernelForge.matvec!(dst, A, x)\n@assert dst ≈ A * x","category":"section"},{"location":"examples/#Row-wise-Reductions","page":"Examples","title":"Row-wise Reductions","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Row sums: y[i] = sum(A[i, :])\nrow_sums = KernelForge.matvec(A, nothing)\n@assert row_sums ≈ vec(sum(A; dims=2))\n\n# Row maximums: y[i] = max_j(A[i, j])\nrow_max = KernelForge.matvec(identity, max, A, nothing)\n@assert row_max ≈ vec(maximum(A; dims=2))\n\n# Row minimums\nrow_min = KernelForge.matvec(identity, min, A, nothing)\n@assert row_min ≈ vec(minimum(A; dims=2))","category":"section"},{"location":"examples/#Custom-Operations","page":"Examples","title":"Custom Operations","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))\ny = KernelForge.matvec((a, b) -> exp(a - b), +, A, x)\n\n# Row-wise dot product with scaling: y[i] = sqrt(sum_j(A[i,j]^2 * x[j]^2))\ny = KernelForge.matvec((a, b) -> a^2 * b^2, +, A, x; g=sqrt)","category":"section"},{"location":"examples/#Custom-Struct-Output","page":"Examples","title":"Custom Struct Output","text":"using KernelForge\nusing CUDA\n\nstruct Vec3\n    x::Float32\n    y::Float32\n    z::Float32\nend\n\n# Map: combine matrix and vector elements\nf(a, b) = Vec3(a * b, a + b, a - b)\n\n# Reduce: component-wise sum\nop(v1::Vec3, v2::Vec3) = Vec3(v1.x + v2.x, v1.y + v2.y, v1.z + v2.z)\n\nA = CUDA.rand(Float32, 200, 500)\nx = CUDA.rand(Float32, 500)\ndst = CuArray{Vec3}(undef, 200)\n\nKernelForge.matvec!(f, op, dst, A, x)","category":"section"},{"location":"examples/#Vector-Matrix-Operations","page":"Examples","title":"Vector-Matrix Operations","text":"","category":"section"},{"location":"examples/#Standard-Vector-Matrix-Multiply","page":"Examples","title":"Standard Vector-Matrix Multiply","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 1000)\n\n# y = x' * A (column-wise weighted sum)\ndst = CUDA.zeros(Float32, 500)\nKernelForge.vecmat!(dst, x, A)\n@assert dst ≈ vec(x' * A)","category":"section"},{"location":"examples/#Column-wise-Reductions","page":"Examples","title":"Column-wise Reductions","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Column sums: y[j] = sum(A[:, j])\ndst = CUDA.zeros(Float32, 500)\nKernelForge.vecmat!(dst, nothing, A)\n@assert dst ≈ vec(sum(A; dims=1))","category":"section"},{"location":"examples/#Pre-allocating-Temporary-Buffers","page":"Examples","title":"Pre-allocating Temporary Buffers","text":"For repeated operations, pre-allocate temporary buffers to avoid allocation overhead:\n\nusing KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 100_000)\ndst = similar(x)\n\n# Pre-allocate for scan\ntmp = KernelForge.get_allocation(KernelForge.scan!, dst, x)\n\n# Reuse in a loop\nfor i in 1:100\n    CUDA.rand!(x)  # new random data\n    KernelForge.scan!(+, dst, x; tmp=tmp)\nend","category":"section"},{"location":"examples/#Complete-Example:-Online-Statistics","page":"Examples","title":"Complete Example: Online Statistics","text":"Compute running mean and variance in a single pass:\n\nusing KernelForge\nusing CUDA\n\nstruct RunningStats\n    n::Float32\n    mean::Float32\n    m2::Float32  # sum of squared deviations\nend\n\n# Welford's online algorithm for combining statistics\nfunction combine(a::RunningStats, b::RunningStats)\n    n = a.n + b.n\n    delta = b.mean - a.mean\n    mean = a.mean + delta * b.n / n\n    m2 = a.m2 + b.m2 + delta^2 * a.n * b.n / n\n    return RunningStats(n, mean, m2)\nend\n\n# Initialize each element as a single observation\ninit(x) = RunningStats(1f0, x, 0f0)\n\nx = CUDA.rand(Float32, 1_000_000)\ndst = CuArray{RunningStats}(undef, length(x))\n\nKernelForge.scan!(init, combine, dst, x)\n\n# Final statistics\nfinal_stats = Array(dst)[end]\nmean = final_stats.mean\nvariance = final_stats.m2 / final_stats.n\nstd = sqrt(variance)","category":"section"},{"location":"#KernelForge.jl","page":"Home","title":"KernelForge.jl","text":"High-performance, portable GPU primitives for Julia. A pure Julia implementation delivering performance competitive with optimized CUDA C++ libraries.\n\nwarning: Experimental Status\nThis package is in an experimental phase. Although extensive testing has been performed, the current implementation does not support views or strided arrays. No bounds checking is performed, which may lead to unexpected behavior with non-contiguous data. Correctness and performance have been validated only on a small NVIDIA RTX 1000.\n\ninfo: Architecture & Contributions\nKernelForge.jl builds on KernelAbstractions.jl for GPU kernel dispatch. However, certain low-level operations—including warp shuffle instructions, vectorized memory access, and memory ordering semantics—are not yet available in KA.jl, so we use KernelIntrinsics.jl for these primitives. As KernelIntrinsics.jl currently supports only CUDA, KernelForge.jl is likewise restricted to CUDA.The core contribution of this package lies in the GPU kernel implementations themselves, designed to be portable once the underlying intrinsics become available on other backends. Extending support to AMD and Intel GPUs would primarily require work in KernelIntrinsics.jl, with minimal adaptations in KernelForge.jl.\n\nnote: Citation\nA paper describing this work is in preparation. If you use this code, please check back for citation details.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"KernelForge\")","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Matrix-vector operations with customizable element-wise and reduction operations\nPrefix scan supporting non-commutative operations\nMap-reduce with custom functions and operators, supporting 1D and 2D reductions\nVectorized copy with configurable load/store widths\nCurrently only for 1D and 2D arrays\nCurrently CUDA-only; cross-platform support via KernelAbstractions.jl planned\nIncludes UnitFloat8, a custom 8-bit floating-point type with range (-1, 1) for testing","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using KernelForge\nusing CUDA\n\n# Prefix scan\nsrc = CUDA.rand(Float32, 10^6)\ndst = similar(src)\nKernelForge.scan!(+, dst, src)\n\n# Matrix-vector multiply\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\ny = KernelForge.matvec(A, x)\n\n# Map-reduce\ntotal = KernelForge.mapreduce(abs2, +, src; to_cpu=true)","category":"section"},{"location":"#Acknowledgments","page":"Home","title":"Acknowledgments","text":"This package builds on the foundation provided by KernelAbstractions.jl and CUDA.jl. The API design draws inspiration from several packages in the Julia ecosystem. Development of the API and documentation was assisted by Claude (Anthropic).","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT","category":"section"}]
}
